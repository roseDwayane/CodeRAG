#!/usr/bin/env python3
"""
Fix Ollama configuration issues
"""

import os
import requests
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def check_current_config():
    """Check current configuration."""
    print("üîç Checking current configuration...")
    
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model_provider = os.getenv("MODEL_PROVIDER", "openai")
    
    print(f"üìã Current configuration:")
    print(f"   MODEL_PROVIDER: {model_provider}")
    print(f"   OLLAMA_BASE_URL: {ollama_url}")
    
    return ollama_url, model_provider

def test_ollama_connection(url):
    """Test connection to Ollama."""
    print(f"\nüîç Testing connection to {url}...")
    
    try:
        response = requests.get(f"{url}/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Connection successful!")
            models = response.json().get("models", [])
            if models:
                print(f"üìã Available models: {[m['name'] for m in models]}")
            else:
                print("‚ö†Ô∏è  No models found")
            return True
        else:
            print(f"‚ùå Connection failed with status code: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Connection refused to {url}")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def fix_env_file():
    """Fix the .env file with correct Ollama configuration."""
    print("\nüîß Fixing .env file...")
    
    env_content = '''# Model Provider Configuration
MODEL_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CHAT_MODEL=llama-3.2-1b-instruct
OLLAMA_EMBEDDING_MODEL=llama-3.2-1b-instruct

# Project Directory Configuration
WATCHED_DIR=.

# FAISS Configuration
FAISS_INDEX_FILE=./coderag_index.faiss
EMBEDDING_DIM=4096
'''
    
    with open('.env', 'w', encoding='utf-8') as f:
        f.write(env_content)
    
    print("‚úÖ .env file updated with correct configuration")
    
    # Reload environment variables
    load_dotenv(override=True)

def check_ollama_status():
    """Check if Ollama is running."""
    print("\nüîç Checking Ollama status...")
    
    try:
        import subprocess
        result = subprocess.run(
            'tasklist /FI "IMAGENAME eq ollama.exe"', 
            shell=True, 
            capture_output=True, 
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        
        if "ollama.exe" in result.stdout:
            print("‚úÖ Ollama process is running")
            return True
        else:
            print("‚ùå Ollama process is not running")
            return False
    except Exception as e:
        print(f"‚ùå Error checking Ollama status: {e}")
        return False

def start_ollama():
    """Start Ollama if not running."""
    print("\nüîÑ Starting Ollama...")
    
    try:
        import subprocess
        subprocess.Popen(
            'ollama serve', 
            shell=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        print("‚úÖ Ollama started in background")
        print("üí° Wait a few seconds for it to fully start")
        return True
    except Exception as e:
        print(f"‚ùå Error starting Ollama: {e}")
        return False

def main():
    """Main function to fix Ollama configuration."""
    print("üîß Fixing Ollama Configuration")
    print("=" * 50)
    
    # Check current configuration
    current_url, current_provider = check_current_config()
    
    # Test current connection
    if not test_ollama_connection(current_url):
        print(f"\n‚ö†Ô∏è  Current configuration is not working")
        
        # Check if Ollama is running
        if not check_ollama_status():
            print("\nüîÑ Ollama is not running, starting it...")
            start_ollama()
        
        # Fix .env file
        fix_env_file()
        
        # Test with correct configuration
        print("\nüîÑ Testing with corrected configuration...")
        if test_ollama_connection("http://localhost:11434"):
            print("\nüéâ Configuration fixed successfully!")
        else:
            print("\n‚ùå Still cannot connect to Ollama")
            print("üí° Please make sure Ollama is installed and running")
    else:
        print("\n‚úÖ Configuration is working correctly!")
    
    print("\n" + "=" * 50)
    print("üìù Next steps:")
    print("   1. Run: python test_ollama_integration.py")
    print("   2. Run: python setup_custom_model_windows.py")
    print("   3. Or test manually: ollama list")

if __name__ == "__main__":
    main()
